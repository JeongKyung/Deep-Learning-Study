{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.datasets import imdb \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Splitting the data into three types(training, validation, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Hold out validation example\n",
    "*One big disadvantage of this validation arises esp when we don't have many data points enough for validation& test data to statistically represent the whole data.  \n",
    "*This problem can be detected if the model performance substantially differs when we re-partition the data.  \n",
    "*K-fold cross validation, Iterated K-fold cross-validation will be good remedies for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8cf7a83e35a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "num_validation_samples=10000\n",
    "np.random.shuffle(data) #it's 'usually' better to first shuffle the data\n",
    "\n",
    "validation_data=data[:num_validation_samples] # create validation dataset\n",
    "data=data[num_validation_samples:]\n",
    "trainindg_data=data[:] #create training dataset\n",
    "\n",
    "#Train the model w/ training set and evaluate the model w/ validation set\n",
    "#Model tuning repeatedly occurs in this stage\n",
    "model=get_model()\n",
    "model.train(training_data)\n",
    "validation_score=model.evaluate(validation_data)\n",
    "\n",
    "#after the hyper-parameter tuning, we re-trainthe model using all data except test data\n",
    "model=get_model()\n",
    "model.train(np.concatenate([training_data,validation_data]))\n",
    "test_score=model.evaludate(test_data) #calculate the final score of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) K-cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4-fold cross validation\n",
    "k=4\n",
    "num_validation_samples=len(data)//k\n",
    "\n",
    "np.random.shuffle(data)\n",
    "validation_scores=[]\n",
    "for fold in range(k):\n",
    "    validation_data=data[num_validation_samples*fold: num_validation_samples*(fold+1)]\n",
    "    training_data=data[:num_validation_samples*fold]+\n",
    "    data[num_validation_samples*(fold+1):] #Linking two lists\n",
    "    \n",
    "    model=get_model() #creating a new, untrained model\n",
    "    \n",
    "    #Train the model w/ training set and evaluate the model w/ validation set\n",
    "    #Model tuning repeatedly occurs in this stage\n",
    "    model.train(training_data)\n",
    "    validation_score=model.evaludate(validation_data)\n",
    "    validation_scores.append(validation_score)\n",
    "    validation_score=np.average(validation_score) #validation score: average of K-fold validation scores\n",
    "\n",
    "#after the hyper-parameter tuning, we re-trainthe model using all data except test data\n",
    "model=get_model()\n",
    "model.train(data)\n",
    "test_score=model.evaluate(test_data) #calculate the final score of interest\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "1)Vectorization: transforming various type of data into tensors  \n",
    "2)Normalization: making different features follow uniform distribution, usually N(0,1)  \n",
    "3)Dealing with omitted values: if test data inclues ommited values, then purposely omit some values in training data.  \n",
    "4)Feature Engineering: utilizing background knowledge to express features in a simple way to solve problem more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overfitting & Underfitting\n",
    " *The fundamental issue of machine learning is finding the balance between optimization and generalization.  \n",
    " *When optimization and generalization improve at the same time, the model is underfitted, and has room for improvement.  \n",
    " *When optimization gets better at the expense of generalization, the model starts to be overfitted.  \n",
    " *The best policty to prevent overfitting (=improve generalization) is to gather more data.  \n",
    " *When gathering more data is not feasible, the second best policy is limiting the model's capacity to absorb or store information, which is called 'regularization'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Reducing the size of Network  \n",
    "*You need to manually increase/decrease to find the optimal number of layers and units.  \n",
    "*While more layers & units ensure fast learning, it also makes the model more vulnerable to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Weight Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "# L2 regularize: adding penalty term to the 'training' loss\n",
    "model=models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), \n",
    "                       activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), \n",
    "                       activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#You can either use L1 regularization, or L1& L2 regularization.\n",
    "## regularizers.l1(0.001)\n",
    "## regularizers.l1_l2(l1=0.001, l2=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ch4_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding two Dropoutlayers and see how wellit prevents overfitting.\n",
    "\n",
    "model=models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5)) #1\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5)) #2\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
